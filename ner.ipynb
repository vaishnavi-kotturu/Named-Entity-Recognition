{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oDTg_G9f4fxP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "UNK = \"UNKKK\"\n",
    "WORDEMBSIZE = 50\n",
    "LABELS = [\"O\",\"B-PER\", \"B-ORG\", \"B-LOC\", \"B-MISC\", \"I-PER\", \"I-ORG\", \"I-LOC\", \"I-MISC\"] \n",
    "num_layers = 3 \n",
    "window_size = 1\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5QnwWMM4KUaW"
   },
   "outputs": [],
   "source": [
    "def read_conll(fstream):\n",
    "    '''\n",
    "    Reads the training data\n",
    "    '''\n",
    "    ret = []\n",
    "\n",
    "    current_toks, current_lbls = [], []\n",
    "    for line in fstream:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0 or line.startswith(\"-DOCSTART-\"):\n",
    "            if len(current_toks) > 0:\n",
    "                assert len(current_toks) == len(current_lbls)\n",
    "                ret.append((current_toks, current_lbls))\n",
    "            current_toks, current_lbls = [], []\n",
    "        else:\n",
    "            arr=line.split()\n",
    "            tok, lbl = arr[0], arr[-1]\n",
    "            if tok.isdigit():\n",
    "                tok = NUM\n",
    "            current_toks.append(tok)\n",
    "            current_lbls.append(lbl)\n",
    "    if len(current_toks) > 0:\n",
    "        assert len(current_toks) == len(current_lbls)\n",
    "        ret.append((current_toks, current_lbls))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z6m3OO_99aa3"
   },
   "outputs": [],
   "source": [
    "def createMapping(data):\n",
    "    '''\n",
    "    Creates unique 'token to number' and 'number to token' mappings\n",
    "    '''\n",
    "    token_to_number = {}\n",
    "    number_to_token = {}\n",
    "    token_to_number = build_dict((word.lower() for sentence,_ in data for word in sentence), offset=1)\n",
    "    token_to_number[\"CASE: aa\"] = len(token_to_number)+1\n",
    "    token_to_number[\"CASE: aA\"] = len(token_to_number)+1\n",
    "    token_to_number[\"CASE: Aa\"] = len(token_to_number)+1\n",
    "    token_to_number[\"CASE: AA\"] = len(token_to_number)+1\n",
    "    token_to_number[START] = len(token_to_number)+1\n",
    "    token_to_number[END] = len(token_to_number)+1\n",
    "    token_to_number[UNK] = len(token_to_number)+1\n",
    "    \n",
    "      \n",
    "    return token_to_number, number_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(words, max_words=None, offset=0):\n",
    "    '''\n",
    "    Returns a dictionary with a maximum number of words mentioned\n",
    "    '''\n",
    "    cnt = Counter(words)\n",
    "    if max_words:\n",
    "        words = cnt.most_common(max_words)\n",
    "    else:\n",
    "        words = cnt.most_common()\n",
    "        \n",
    "    return {word: offset+i for i, (word, _) in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findCase(word):\n",
    "    '''\n",
    "    Returns the casing of a word\n",
    "    ''' \n",
    "    if len(word) == 0: return word\n",
    "    \n",
    "    # all lowercase\n",
    "    if word.islower(): return \"CASE: aa\"\n",
    "    # all uppercase\n",
    "    elif word.isupper(): return \"CASE: AA\"\n",
    "    # starts with capital\n",
    "    elif word[0].isupper(): return \"CASE: Aa\"\n",
    "    # has non-initial capital\n",
    "    else: return \"CASE: aA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9H6pFXPgCvbb"
   },
   "outputs": [],
   "source": [
    "def embeddingLayer(fstream, token_to_num):\n",
    "    '''\n",
    "    Returns the Embedding Layer Matrix\n",
    "    '''\n",
    "    embeddings_dict = {}\n",
    "    for line in fstream:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "    emb = np.zeros((len(token_to_num) + 1, 50))\n",
    "    emb[0] = 0.\n",
    "    for word, vec in embeddings_dict.items():\n",
    "        if word in token_to_num:\n",
    "            emb[token_to_num[word]] = vec\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BXSylvHcatDp"
   },
   "outputs": [],
   "source": [
    "def oneHot(n_dim, i):\n",
    "    '''\n",
    "    Returns a one Hot encoded array\n",
    "    '''\n",
    "    if isinstance(i, int):\n",
    "        \n",
    "        ret = np.zeros(n_dim)\n",
    "        ret[i] = 1.0\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XObxpIqcP77T"
   },
   "outputs": [],
   "source": [
    "def makeWindowedData(dat, size_wind, embedding_layer, token_to_number, LABELS):\n",
    "    '''\n",
    "    Returns windowed word embeddings \n",
    "    '''\n",
    "    final = list()\n",
    "    for el in dat:\n",
    "      sentence = el[0]\n",
    "      lab = el[1]\n",
    "      sentence = [START]*size_wind + sentence + [END]*size_wind\n",
    "      for i in range(len(sentence)):\n",
    "            temp = sentence[i].lower()\n",
    "            if temp not in token_to_number:\n",
    "                sentence[i] = UNK\n",
    "                temp = UNK\n",
    "            sentence[i] = np.hstack((embedding_layer[token_to_number[temp]],embedding_layer[token_to_number[findCase(sentence[i])]]))\n",
    "      k = 0\n",
    "      for i in range(size_wind, len(sentence)-size_wind):\n",
    "          res = list()\n",
    "          for j in range(i-size_wind, i+size_wind+1):\n",
    "              res.extend(sentence[j]) \n",
    "          final.append(np.array([np.array(res),oneHot(len(LABELS), LABELS.index(lab[k]))]))\n",
    "          k += 1\n",
    "    return np.array(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classFreqDict(word_emb):\n",
    "    '''\n",
    "    Returns the Frequency of the Class of the given word\n",
    "    '''\n",
    "    y_true=[]\n",
    "    freq_dict = {}\n",
    "    total = len(word_emb)\n",
    "    for el in word_emb:\n",
    "        t_label = el[1]\n",
    "        x = t_label.tolist()\n",
    "        val = LABELS[x.index(max(x))]\n",
    "        if val not in freq_dict:\n",
    "            freq_dict[val] = 1\n",
    "        else:\n",
    "            freq_dict[val] += 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_weights(n,m): # m:input layer nodes, n:output layer nodes\n",
    "    '''\n",
    "    Xavier Glorot Initialization of Weights\n",
    "    '''\n",
    "    r = np.sqrt(6/(m+n))\n",
    "    weights = np.random.uniform(-r,r,size=(n,m))\n",
    "    return weights\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    return A\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return np.array(A, dtype = np.float128)\n",
    "\n",
    "def softmax(x):\n",
    "    b = x.max()\n",
    "    y = np.exp(x - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "def cross_entropy_loss(output, Y_batch, freq_dict, total, hidden_weights, lambd):\n",
    "    '''\n",
    "    Returns the Weighted Cross Entropy Loss with Regularization\n",
    "    '''\n",
    "    m = Y_batch.shape[1]\n",
    "    freq = []\n",
    "    for i in range(len(Y_batch)):\n",
    "        t_label = Y_batch[i]\n",
    "        x = t_label.tolist()\n",
    "        val = LABELS[x.index(max(x))]\n",
    "        freq.append(freq_dict[val]/total)\n",
    "    freq = np.array(freq)\n",
    "    freq = np.reshape(freq,(-1,1))\n",
    "    out_sum = np.sum(np.sum(np.multiply(freq,Y_batch) * np.where(output != 0, np.log(output)+.001, 0), axis=0, keepdims=True), axis=1)\n",
    "    loss = -(1/m)*out_sum\n",
    "    \n",
    "    L = len(hidden_weights)\n",
    "    L2_regularization_cost = 0\n",
    "    for i in range(1,L):\n",
    "        L2_regularization_cost += np.sum(np.square(hidden_weights[i]))\n",
    "\n",
    "    L2_regularization_cost = L2_regularization_cost*(lambd/(2*m))\n",
    "    loss += L2_regularization_cost\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwdprop(X_batch, hidden_weights, bias, hidden_layers_activation_fn=\"relu\"):\n",
    "    '''\n",
    "    Performs forward propagation part of the Network\n",
    "    '''\n",
    "    A = X_batch                           \n",
    "    caches = [] \n",
    "    Z = []\n",
    "    A_last = []\n",
    "    L = len(hidden_weights)       \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A = []\n",
    "        for i in range(len(X_batch)):\n",
    "            z = np.dot(hidden_weights[l], np.reshape(A_prev[i],(-1,1))) + bias[l]\n",
    "            a = hidden_layers_activation_fn(z)\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "        cache =(np.array(A_prev,dtype=np.float128), hidden_weights[l],bias[l],np.array(Z,dtype=np.float128))\n",
    "        A = np.array(A)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    A = np.squeeze(A)\n",
    "    for i in range(len(X_batch)):\n",
    "        z = np.dot(hidden_weights[L],np.reshape(A[i], (-1,1))) + bias[L]\n",
    "        a = softmax(np.array(z, dtype=np.float128))\n",
    "        Z.append(z)\n",
    "        A_last.append(a)\n",
    "    \n",
    "    cache = (np.array(A), hidden_weights[L], bias[L], np.array(Z))\n",
    "    caches.append(cache)\n",
    "\n",
    "    return np.squeeze(np.array(A_last,dtype=np.float128)), caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backprop(dA, Z):\n",
    "    A, Z = sigmoid(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def tanh_backprop(dA, Z):\n",
    "    A, Z = tanh(Z)\n",
    "    dZ = dA * (1 - np.square(A))\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backprop(dA, Z):\n",
    "    A = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "\n",
    "    return np.array(dZ, dtype = np.float128)\n",
    "\n",
    "def softmax_backprop(target, predicted):\n",
    "    return predicted - target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def bwd_step(dZ, A_prev, W, b):\n",
    "    '''\n",
    "    Performs one step of Backward Propagation\n",
    "    '''\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return np.array(dW,dtype = np.float128), np.array(db,dtype = np.float128), np.array(dA_prev, dtype = np.float128)\n",
    "\n",
    "def bwdprop(output, Y_batch, caches, hidden_weights, lambd, hidden_layers_activation_fn=\"relu\"):\n",
    "    '''\n",
    "    Peforms Backward Propagation\n",
    "    '''\n",
    "    m= Y_batch.shape[1]\n",
    "    \n",
    "    Y_batch = Y_batch.reshape(output.shape)\n",
    "    L = len(caches)\n",
    "    A_prev, W, b, Z =caches[L-1]\n",
    "    dW = {}\n",
    "    dB = {}\n",
    "    dA = {}\n",
    "    \n",
    "    dZ = softmax_backprop(Y_batch, output)\n",
    "    dW[L] = []\n",
    "    dB[L] = []\n",
    "    dA[L-1] = []\n",
    "    \n",
    "    for i in range(len(dZ)):\n",
    "        dw = np.dot(np.reshape(dZ[i],(-1,1)), np.transpose(np.reshape(A_prev[i],(-1,1)))) + (lambd/m)*hidden_weights[L]\n",
    "        db = np.reshape(dZ[i],(-1,1))\n",
    "        da = np.dot(W.T, np.reshape(dZ[i],(-1,1)))\n",
    "        dW[L].append(dw)\n",
    "        dB[L].append(db)\n",
    "        dA[L-1].append(da)\n",
    "\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        cache = caches[l - 1]\n",
    "        A_prev, W, b, Z =cache\n",
    "        Z = np.squeeze(Z)\n",
    "        dA[l] = np.array(dA[l])\n",
    "        dW[l] = []\n",
    "        dB[l] = []\n",
    "        dA[l-1] = []\n",
    "\n",
    "        for i in range(len(dZ)):\n",
    "            dz = relu_backprop(dA[l][i], np.reshape(Z[i],(-1,1)))\n",
    "            dz = np.array(dz)\n",
    "            dw, db, da = bwd_step(dz, np.reshape(A_prev[i],(-1,1)), W, b) \n",
    "            dw += (lambd/m)*hidden_weights[l]\n",
    "            \n",
    "            if hidden_layers_activation_fn == \"sigmoid\":\n",
    "                dz = sigmoid_backprop(dA[l][i], np.reshape(Z[i],(-1,1)))\n",
    "                dw, db, da = bwd_step(dz, np.transpose(np.reshape(A_prev[i],(-1,1))), W, b) \n",
    "\n",
    "            elif hidden_layers_activation_fn == \"tanh\":\n",
    "                dz = tanh_backprop(dA[l][i],  np.reshape(Z[i],(-1,1)))\n",
    "                dw, db, da = bwd_step(dz, np.transpose(np.reshape(A_prev[i],(-1,1))), W, b) \n",
    "\n",
    "            elif hidden_layers_activation_fn == \"relu\": \n",
    "                dz = relu_backprop(dA[l][i], np.reshape(Z[i],(-1,1)))\n",
    "                dz = np.array(dz)\n",
    "                dw, db, da = bwd_step(dz, np.transpose(np.reshape(A_prev[i],(-1,1))), W, b) \n",
    "        \n",
    "            \n",
    "            dW[l].append(dw)\n",
    "            dB[l].append(db)\n",
    "            dA[l-1].append(da)\n",
    "    return dW,dB\n",
    "\n",
    "\n",
    "def update_parameters(hidden_weights, b, dW, db, alpha):\n",
    "    '''\n",
    "    Returns the updated weights\n",
    "    '''\n",
    "    L = len(hidden_weights)\n",
    "\n",
    "    length = len(dW[1])\n",
    "    for l in range(1, L + 1):\n",
    "        dweights = np.array(dW[l])\n",
    "        dbias = np.array(db[l])\n",
    "\n",
    "        for i in range(1,length):\n",
    "            \n",
    "            hidden_weights[l] -= alpha*(dweights[i])\n",
    "            b[l] -= alpha*dbias[i]\n",
    "        \n",
    "    return hidden_weights,b\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuralNetwork_train(data, vocabulary_size, num_layers, H, embed_size, alpha, batch_size, epochs,window_size, activation_fn, freq_dict, lambd):\n",
    "    '''\n",
    "    Trains the Neural Network\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(data)):\n",
    "        X.append(data[i][0])\n",
    "        Y.append(data[i][1])\n",
    "    X = np.array(X, dtype=np.float128)\n",
    "    Y = np.array(Y, dtype=np.float128)\n",
    "    \n",
    "    hidden_weights ={}\n",
    "    b = {}\n",
    "    \n",
    "    for i in range(1,num_layers):\n",
    "        hidden_weights[i] = initialize_weights(H[i],H[i-1]) \n",
    "        b[i] = np.zeros((H[i],1), dtype=np.float128)\n",
    "    no_windows = len(data)\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cost = 0\n",
    "        store_val = []\n",
    "        for i in range(0,no_windows-1,batch_size):\n",
    "            X_batch = X[i:i+batch_size,:]\n",
    "            Y_batch = Y[i:i+batch_size,:]\n",
    "\n",
    "            output, caches = fwdprop(X_batch, hidden_weights,b,activation_fn)\n",
    "            loss = cross_entropy_loss(output, Y_batch,freq_dict,len(data),hidden_weights, lambd)\n",
    "            dW, db = bwdprop(output, Y_batch, caches, hidden_weights, lambd, activation_fn)\n",
    "            store_val.append(output)\n",
    "            hidden_weights,b = update_parameters(hidden_weights, b, dW, db, alpha)\n",
    "            cost += np.squeeze(loss)\n",
    "        costs.append(cost)\n",
    "    plt.plot(np.arange(epochs), costs)\n",
    "    plt.xlabel('# of epochs')\n",
    "    plt.ylabel('cost')\n",
    "    return store_val,hidden_weights,b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall_f1score(y_true, predicted, LABELS):\n",
    "    '''\n",
    "    Prints the Precision, Recall and F1-score\n",
    "    '''\n",
    "    tp = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    fp = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    tn = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    fn = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    prec = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    rec = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    f1 = {\"O\" : 0,\"B-PER\": 0, \"B-ORG\": 0, \"B-LOC\": 0, \"B-MISC\": 0, \"I-PER\": 0, \"I-ORG\": 0, \"I-LOC\": 0, \"I-MISC\": 0}\n",
    "    tp_total = 0\n",
    "    fp_total = 0\n",
    "    tn_total = 0\n",
    "    fn_total = 0\n",
    "    for label in LABELS:\n",
    "        for i in range(len(y_true)):\n",
    "            if label == y_true[i]:\n",
    "                if y_true[i]==predicted[i]:\n",
    "                    tp[label] += 1\n",
    "                    tp_total += 1\n",
    "                else:\n",
    "                    fn[label] += 1\n",
    "                    fn_total += 1\n",
    "            elif label == predicted[i]:\n",
    "                fp[label] += 1\n",
    "                fp_total += 1\n",
    "            else:\n",
    "                tn[label] += 1\n",
    "                tn_total += 1\n",
    "        prec[label] = tp[label] / (tp[label] + fp[label])\n",
    "        rec[label] = tp[label] / (tp[label] + fn[label])\n",
    "        f1[label] = 2 * prec[label] * rec[label] / (prec[label]+rec[label])\n",
    "    prec3=0\n",
    "    rec3=0\n",
    "    f1_3=0\n",
    "    for label in LABELS:\n",
    "        weight = y_true.count(label)/len(y_true)\n",
    "        prec3 += weight * prec[label]\n",
    "        rec3 += weight * rec[label]\n",
    "        f1_3 += weight * f1[label]\n",
    "    for label in LABELS:\n",
    "        print('Label :',label,'\\nActual :',y_true.count(label),'Predicted :', predicted.count(label),'\\n')\n",
    "    print('Precision :',prec3)\n",
    "    print('Recall :',rec3)\n",
    "    print('F1-Score :',f1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w,b):\n",
    "    '''\n",
    "    Runs the model on the test set and prints the class-wise data and the evaluation metrics values\n",
    "    '''\n",
    "    test_data = read_conll(open('test.txt','r'))\n",
    "    token_to_number_test, number_to_token_test = createMapping(test_data)\n",
    "    embedding_layer = embeddingLayer(open('embeddings.txt'),token_to_number_test)\n",
    "    word_emb_test = makeWindowedData(test_data, window_size, embedding_layer, token_to_number_test, LABELS)\n",
    "    X = []\n",
    "    Y= []\n",
    "    for i in range(len(word_emb_test)):\n",
    "            X.append(word_emb_test[i][0])\n",
    "            Y.append(word_emb_test[i][1])\n",
    "    output, caches = fwdprop(X,w,b,relu)\n",
    "    output = output.tolist()\n",
    "    predicted = []\n",
    "    for k in range(len(output)):\n",
    "            predicted.append(LABELS[output[k].index(max(output[k]))])\n",
    "    y_true = []\n",
    "    for el in word_emb_test:\n",
    "        t_label=el[1]\n",
    "        x=t_label.tolist()\n",
    "        y_true.append(LABELS[x.index(max(x))])\n",
    "    precision_recall_f1score(y_true, predicted, LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RyaVXtnQLMwI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Training\n",
    "# #Uncomment this cell to Train the model\n",
    "# data = read_conll(open('train.txt','r'))\n",
    "# H =[300,150,9]\n",
    "# token_to_number, num2tok = createMapping(data)\n",
    "# embedding_layer = embeddingLayer(open('embeddings.txt'), token_to_number)\n",
    "# word_emb = makeWindowedData(data, window_size, embedding_layer, token_to_number, LABELS)\n",
    "# freq_dict = classFreqDict(word_emb)\n",
    "# y_pred, w, b = neuralNetwork_train(word_emb, len(token_to_number), num_layers, H, WORDEMBSIZE, 0.001, 32, 20, window_size, relu, freq_dict, 0.2)\n",
    "# np.save('weights', w)\n",
    "# np.save('bias', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : O \n",
      "Actual : 38323 Predicted : 37006 \n",
      "\n",
      "Label : B-PER \n",
      "Actual : 1617 Predicted : 1096 \n",
      "\n",
      "Label : B-ORG \n",
      "Actual : 1661 Predicted : 867 \n",
      "\n",
      "Label : B-LOC \n",
      "Actual : 1668 Predicted : 2949 \n",
      "\n",
      "Label : B-MISC \n",
      "Actual : 702 Predicted : 1303 \n",
      "\n",
      "Label : I-PER \n",
      "Actual : 1156 Predicted : 1582 \n",
      "\n",
      "Label : I-ORG \n",
      "Actual : 835 Predicted : 485 \n",
      "\n",
      "Label : I-LOC \n",
      "Actual : 257 Predicted : 843 \n",
      "\n",
      "Label : I-MISC \n",
      "Actual : 216 Predicted : 304 \n",
      "\n",
      "Precision : 0.8429413818807842\n",
      "Recall : 0.8230644987617101\n",
      "F1-Score : 0.8281321474207554\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "w = np.load('trained_weights.npy', allow_pickle=True)\n",
    "b = np.load('trained_bias.npy', allow_pickle=True)\n",
    "weights = {}\n",
    "bias = {}\n",
    "for i in range(1, num_layers):\n",
    "    weights[i] = w.item().get(i)\n",
    "    bias[i] = b.item().get(i)\n",
    "predict(weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sel_Topics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
